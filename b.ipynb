{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ha200\\MyWork\\Study\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.3 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\ha200\\MyWork\\Study\\Năm 3 Học Kì 1\\AI\\Human-Action-Recognition-Using-Detectron2-And-Lstm\\models\\saved_model.ckpt`\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import warnings\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from src.lstm import ActionClassificationLSTM\n",
    "\n",
    "# Load the pre-trained LSTM model\n",
    "lstm_classifier = ActionClassificationLSTM.load_from_checkpoint(\"models/saved_model.ckpt\")\n",
    "lstm_classifier.eval()\n",
    "\n",
    "# Ignore warnings from protobuf\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf')\n",
    "\n",
    "# MediaPipe pose initialization\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Define color\n",
    "WHITE_COLOR = (255, 255, 255)\n",
    "GREEN_COLOR = (0, 255, 0)\n",
    "\n",
    "LABELS = {\n",
    "    0: \"JUMPING\",\n",
    "    1: \"JUMPING_JACKS\",\n",
    "    2: \"BOXING\",\n",
    "    3: \"WAVING_2HANDS\",\n",
    "    4: \"WAVING_1HAND\",\n",
    "    5: \"CLAPPING_HANDS\",\n",
    "    6: \"RUNNING\"\n",
    "}\n",
    "\n",
    "WINDOW_SIZE = 30\n",
    "SKIP_FRAME_COUNT = 0\n",
    "\n",
    "def pose_detector(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        keypoints_indices = [0, 2, 5, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 29, 30, 31, 32]\n",
    "        keypoints = [(results.pose_landmarks.landmark[i].x * frame.shape[1],\n",
    "                      results.pose_landmarks.landmark[i].y * frame.shape[0]) for i in keypoints_indices]\n",
    "        return keypoints\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def draw_line(image, p1, p2, color):\n",
    "    if isinstance(p1, tuple) and isinstance(p2, tuple) and len(p1) == 2 and len(p2) == 2:\n",
    "        p1 = (int(p1[0]), int(p1[1]))\n",
    "        p2 = (int(p2[0]), int(p2[1]))\n",
    "        cv2.line(image, p1, p2, color, thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def analyse_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    file_name = ntpath.basename(video_path)\n",
    "    vid_writer = cv2.VideoWriter(f'res_{file_name}', fourcc, 30, (width, height))\n",
    "\n",
    "    buffer_window = []\n",
    "    label = None\n",
    "    counter = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = frame.copy()\n",
    "        if counter % (SKIP_FRAME_COUNT + 1) == 0:\n",
    "            keypoints = pose_detector(frame)\n",
    "\n",
    "            if len(keypoints) >= 17: \n",
    "                draw_line(frame, keypoints[3], keypoints[4], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[3], keypoints[5], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[5], keypoints[7], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[3], keypoints[9], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[9], keypoints[11], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[11], keypoints[13], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[4], keypoints[6], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[6], keypoints[8], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[4], keypoints[10], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[10], keypoints[12], WHITE_COLOR)\n",
    "                draw_line(frame, keypoints[12], keypoints[14], WHITE_COLOR)\n",
    "\n",
    "                for (x, y) in keypoints:\n",
    "                    cv2.circle(frame, (int(x), int(y)), 5, GREEN_COLOR, -1)\n",
    "\n",
    "\n",
    "            if len(keypoints) == 17:\n",
    "                features = [coord for point in keypoints for coord in point]\n",
    "\n",
    "                if len(buffer_window) < WINDOW_SIZE:\n",
    "                    buffer_window.append(features)\n",
    "                else:\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model_input = torch.Tensor(np.array(buffer_window, dtype=np.float32))\n",
    "                    model_input = torch.unsqueeze(model_input, dim=0).to(device)\n",
    "                    y_pred = lstm_classifier(model_input)\n",
    "                    prob = F.softmax(y_pred, dim=1)\n",
    "                    pred_index = prob.data.max(dim=1)[1]\n",
    "                    buffer_window.pop(0)\n",
    "                    buffer_window.append(features)\n",
    "                    label = LABELS[pred_index.cpu().numpy()[0]]\n",
    "\n",
    "                \n",
    "\n",
    "                if label:\n",
    "                    cv2.putText(frame, f'Action: {label}', (width - 400, height - 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 2)\n",
    "\n",
    "        counter += 1\n",
    "        vid_writer.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    vid_writer.release()\n",
    "    end = time.time()\n",
    "    print(\"Video processing finished in \", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing finished in  22.668447971343994\n",
      "Video processing completed. The result is saved in the output video.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Đường dẫn đến video mà bạn muốn phân tích\n",
    "    video_path = \"sample_video.mp4\"  # Thay đổi đường dẫn này thành video của bạn\n",
    "\n",
    "    # Gọi hàm phân tích video\n",
    "    analyse_video(video_path)\n",
    "\n",
    "    print(\"Video processing completed. The result is saved in the output video.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
